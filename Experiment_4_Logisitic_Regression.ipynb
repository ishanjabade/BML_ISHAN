{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwJI88d+UitTHaJGj3isP1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishanjabade/BML_ISHAN/blob/main/Experiment_4_Logisitic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxosvj5pDJyA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment 4: Build a Logisitic Regression Model on suitable dataset.**"
      ],
      "metadata": {
        "id": "8bgqeXd9DvXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Used: Heart Disease Dataset**"
      ],
      "metadata": {
        "id": "F0lPsNU8DwzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load And Prepare Dataset"
      ],
      "metadata": {
        "id": "4owbSbN8Dzi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/heart.csv')\n",
        "\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "\n",
        "# Check columns\n",
        "print(df.columns)\n",
        "\n",
        "# Target column is \"target\", convert to 0/1 (already 0/1)\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5Q5q5PMD16R",
        "outputId": "f64acd60-f2f9-487f-80af-436d78191446"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "(1025, 14)\n",
            "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
            "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
            "      dtype='object')\n",
            "Training shape: (820, 13)\n",
            "Test shape: (205, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Manual Implementation**"
      ],
      "metadata": {
        "id": "_Lrmkjg_FwFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Kaggle heart.csv\n",
        "df = pd.read_csv('/content/heart.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"target\", axis=1).values\n",
        "y = df[\"target\"].values.reshape(-1, 1)\n",
        "\n",
        "# Scale features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Logistic Regression from scratch\n",
        "def train_logistic_regression(X, y, lr=0.01, epochs=2000):\n",
        "    m, n = X.shape\n",
        "    W = np.zeros((n, 1))\n",
        "    b = 0\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # Forward pass\n",
        "        z = np.dot(X, W) + b\n",
        "        y_pred = sigmoid(z)\n",
        "\n",
        "        # Loss: binary cross entropy\n",
        "        cost = -(1/m) * np.sum(y * np.log(y_pred + 1e-9) +\n",
        "                               (1-y) * np.log(1-y_pred + 1e-9))\n",
        "\n",
        "        # Gradients\n",
        "        dW = (1/m) * np.dot(X.T, (y_pred - y))\n",
        "        db = (1/m) * np.sum(y_pred - y)\n",
        "\n",
        "        # Update\n",
        "        W -= lr * dW\n",
        "        b -= lr * db\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"Epoch {i}, Loss = {cost:.4f}\")\n",
        "\n",
        "    return W, b\n",
        "\n",
        "# Train\n",
        "W, b = train_logistic_regression(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "def predict(X, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = sigmoid(z)\n",
        "    return (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Test accuracy\n",
        "y_pred_test = predict(X_test, W, b)\n",
        "accuracy = np.mean(y_pred_test == y_test)\n",
        "\n",
        "print(\"Manual Logistic Regression Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stbSu4I8Fxng",
        "outputId": "63c9ed37-0800-486d-a20d-b26ffef20079"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss = 0.6931\n",
            "Epoch 200, Loss = 0.4337\n",
            "Epoch 400, Loss = 0.3826\n",
            "Epoch 600, Loss = 0.3625\n",
            "Epoch 800, Loss = 0.3520\n",
            "Epoch 1000, Loss = 0.3458\n",
            "Epoch 1200, Loss = 0.3417\n",
            "Epoch 1400, Loss = 0.3388\n",
            "Epoch 1600, Loss = 0.3368\n",
            "Epoch 1800, Loss = 0.3353\n",
            "Manual Logistic Regression Accuracy: 0.7902439024390244\n"
          ]
        }
      ]
    }
  ]
}